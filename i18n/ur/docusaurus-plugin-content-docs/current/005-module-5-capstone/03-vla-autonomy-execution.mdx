---
title: "Capstone: VLA Command Execution & Full Autonomy"
sidebar_label: "VLA Command Execution & Autonomy"
sidebar_position: 3
description: "Execute voice commands through the full integrated system for autonomous humanoid behavior"
keywords: [capstone, vla, autonomy, voice control, integration, robotics]
---

# Capstone: VLA Command Execution & Full Autonomy

## Introduction

In this final capstone lesson, you'll bring together the complete VLA (Vision-Language-Action) pipeline with your integrated perception and control system. This creates a fully autonomous humanoid robot that can understand voice commands, perceive its environment, plan actions, and execute them in simulation.

## Complete VLA Architecture

The full VLA system integrates:

### Voice Processing
- **Speech Recognition**: Whisper converts speech to text
- **Natural Language Understanding**: LLM parses commands
- **Action Planning**: Structured action sequences generated

### Perception System
- **Multi-sensor Fusion**: Camera, LiDAR, IMU data combined
- **Object Detection**: Identify targets in environment
- **State Estimation**: Track robot position and status

### Action Execution
- **Navigation**: Move to specified locations
- **Manipulation**: Interact with objects
- **Feedback**: Report results to user

## Hands-On Lab: Complete VLA System

Let's create the complete integrated system that combines all components.

### Complete VLA Integration Node

```python title="code-examples/capstone-module/full_vla_integration.py"
import rclpy
from rclpy.node import Node
from rclpy.qos import QoSProfile
from rclpy.action import ActionClient

from sensor_msgs.msg import Image, LaserScan, Imu, JointState
from geometry_msgs.msg import Twist, PoseStamped
from nav_msgs.msg import Odometry
from std_msgs.msg import String
from std_msgs.msg import Bool

import pyaudio
import wave
import numpy as np
import openai
from openai import OpenAI
import json
import time
import threading
import queue
from cv_bridge import CvBridge
import cv2

class FullVLAIntegration(Node):
    def __init__(self):
        super().__init__('full_vla_integration')

        # Initialize components
        self.bridge = CvBridge()
        self.audio = pyaudio.PyAudio()

        # VLA components
        self.whisper_transcriber = WhisperTranscriber()
        self.llm_planner = LLMActionPlanner()

        # ROS2 publishers and subscribers
        qos_profile = QoSProfile(depth=10)

        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)
        self.speech_pub = self.create_publisher(String, '/speech_output', 10)
        self.vla_command_pub = self.create_publisher(String, '/vla/action_plan', 10)

        # Sensor subscribers
        self.image_sub = self.create_subscription(Image, '/camera/image_raw', self.image_callback, qos_profile)
        self.scan_sub = self.create_subscription(LaserScan, '/scan', self.scan_callback, qos_profile)
        self.imu_sub = self.create_subscription(Imu, '/imu/data', self.imu_callback, qos_profile)
        self.joint_sub = self.create_subscription(JointState, '/joint_states', self.joint_callback, qos_profile)
        self.odom_sub = self.create_subscription(Odometry, '/odom', self.odom_callback, qos_profile)

        # VLA command subscriber
        self.vla_sub = self.create_subscription(String, '/vla/action_plan', self.vla_command_callback, qos_profile)

        # Internal state
        self.latest_image = None
        self.latest_scan = None
        self.robot_pose = None
        self.is_executing = False
        self.current_action_index = 0

        # VLA state
        self.vla_enabled = True
        self.vla_listening = False

        # Control parameters
        self.linear_speed = 0.5
        self.angular_speed = 0.8

        # Start VLA listening thread
        self.vla_thread = threading.Thread(target=self.vla_listening_loop, daemon=True)
        self.vla_thread.start()

        self.get_logger().info('Full VLA Integration System initialized')

    def image_callback(self, msg):
        """Process camera image"""
        try:
            self.latest_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")
        except Exception as e:
            self.get_logger().error(f'Error processing image: {e}')

    def scan_callback(self, msg):
        """Process laser scan"""
        self.latest_scan = msg

    def imu_callback(self, msg):
        """Process IMU data"""
        pass  # Store if needed for state estimation

    def joint_callback(self, msg):
        """Process joint states"""
        pass  # Store if needed for control

    def odom_callback(self, msg):
        """Process odometry"""
        self.robot_pose = msg.pose.pose

    def vla_command_callback(self, msg):
        """Process VLA action plan"""
        if self.is_executing:
            self.get_logger().warn('Currently executing an action plan, ignoring new plan')
            return

        try:
            action_plan = json.loads(msg.data)
            self.get_logger().info(f'Received action plan with {len(action_plan.get("actions", []))} actions')

            # Execute the action plan
            self.execute_action_plan(action_plan)

        except json.JSONDecodeError:
            self.get_logger().error('Invalid JSON in action plan')
        except Exception as e:
            self.get_logger().error(f'Error processing action plan: {e}')

    def vla_listening_loop(self):
        """Continuous listening loop for voice commands"""
        self.get_logger().info('Starting VLA listening loop')

        while rclpy.ok() and self.vla_enabled:
            if not self.vla_listening and not self.is_executing:
                # Start listening for voice command
                self.get_logger().info('Listening for voice command...')

                try:
                    # Record and transcribe
                    self.get_logger().info('Say your command now...')
                    text = self.whisper_transcriber.transcribe_from_microphone()

                    if text.strip():
                        self.get_logger().info(f'Recognized: {text}')

                        # Plan actions using LLM
                        robot_state = self.get_robot_state()
                        action_plan = self.llm_planner.plan_actions(text, robot_state)

                        if action_plan:
                            # Publish action plan for execution
                            action_plan_msg = String()
                            action_plan_msg.data = json.dumps(action_plan)
                            self.vla_command_pub.publish(action_plan_msg)

                            self.get_logger().info('Action plan published for execution')
                        else:
                            self.get_logger().error('Failed to generate action plan')
                            # Report failure to user
                            self.speak_response("Sorry, I couldn't understand that command.")

                except Exception as e:
                    self.get_logger().error(f'Error in VLA listening loop: {e}')

            time.sleep(0.1)  # Small delay to prevent busy waiting

    def get_robot_state(self):
        """Get current robot state for LLM context"""
        state = {
            "position": [0, 0, 0],  # Would get from odometry
            "orientation": [0, 0, 0, 1],  # Would get from IMU/odometry
            "gripper": "open",  # Would get from joint states
            "environment": "indoor",  # Would get from perception
        }

        if self.robot_pose:
            pos = self.robot_pose.position
            orient = self.robot_pose.orientation
            state["position"] = [pos.x, pos.y, pos.z]
            state["orientation"] = [orient.x, orient.y, orient.z, orient.w]

        return state

    def execute_action_plan(self, action_plan: dict):
        """Execute the planned actions"""
        if not action_plan or 'actions' not in action_plan:
            self.get_logger().error('No valid action plan to execute')
            return

        self.is_executing = True
        self.current_action_index = 0
        actions = action_plan['actions']

        self.get_logger().info(f'Starting execution of {len(actions)} actions')

        for i, action in enumerate(actions):
            if not self.vla_enabled:  # Check if system is still enabled
                self.get_logger().info('VLA system disabled during execution')
                break

            self.get_logger().info(f'Executing action {i+1}/{len(actions)}: {action.get("description", "Unknown")}')

            success = self.execute_single_action(action)
            if not success:
                self.get_logger().error(f'Action {i+1} failed: {action.get("description", "Unknown")}')
                self.speak_response(f"Sorry, I couldn't complete the action: {action.get('description', 'Unknown')}")
                break

            self.get_logger().info(f'Action {i+1} completed successfully')
            self.current_action_index = i + 1

        self.get_logger().info('Action execution completed')
        self.is_executing = False

    def execute_single_action(self, action: dict) -> bool:
        """Execute a single action"""
        action_type = action.get('action_type', '')
        parameters = action.get('parameters', {})

        if action_type == 'move':
            return self.execute_move_action(parameters)
        elif action_type == 'navigate':
            return self.execute_navigate_action(parameters)
        elif action_type == 'grip':
            return self.execute_grip_action(parameters)
        elif action_type == 'release':
            return self.execute_release_action(parameters)
        elif action_type == 'speak':
            return self.execute_speak_action(parameters)
        elif action_type == 'wait':
            return self.execute_wait_action(parameters)
        elif action_type == 'find_object':
            return self.execute_find_object_action(parameters)
        else:
            self.get_logger().warn(f'Unknown action type: {action_type}')
            return False

    def execute_move_action(self, parameters: dict) -> bool:
        """Execute a move action"""
        try:
            x_vel = parameters.get('linear_x', 0.0)
            y_vel = parameters.get('linear_y', 0.0)
            z_vel = parameters.get('linear_z', 0.0)
            ang_vel = parameters.get('angular_z', 0.0)
            duration = parameters.get('duration', 1.0)

            twist_msg = Twist()
            twist_msg.linear.x = x_vel
            twist_msg.linear.y = y_vel
            twist_msg.linear.z = z_vel
            twist_msg.angular.z = ang_vel

            start_time = time.time()
            end_time = start_time + duration

            self.get_logger().info(f'Moving with linear=({x_vel}, {y_vel}, {z_vel}), angular=({ang_vel}) for {duration}s')

            while time.time() < end_time and not self.is_shutting_down():
                self.cmd_vel_pub.publish(twist_msg)
                time.sleep(0.1)

            # Stop the robot
            stop_msg = Twist()
            self.cmd_vel_pub.publish(stop_msg)

            return True

        except Exception as e:
            self.get_logger().error(f'Error executing move action: {e}')
            return False

    def execute_navigate_action(self, parameters: dict) -> bool:
        """Execute a navigation action (simplified)"""
        try:
            x = parameters.get('x', 0.0)
            y = parameters.get('y', 0.0)

            # In a real system, this would use navigation2
            # For simulation, we'll just move in that direction for a while
            self.get_logger().info(f'Navigating toward: ({x}, {y})')

            # Calculate direction vector
            if self.robot_pose:
                dx = x - self.robot_pose.position.x
                dy = y - self.robot_pose.position.y
                distance = np.sqrt(dx*dx + dy*dy)

                if distance > 0.1:  # If not already at destination
                    linear_vel = min(self.linear_speed, distance)
                    angular_vel = np.arctan2(dy, dx)  # Simplified heading

                    # Move toward target
                    twist_msg = Twist()
                    twist_msg.linear.x = linear_vel
                    twist_msg.angular.z = angular_vel

                    # Move for a calculated time
                    move_time = distance / self.linear_speed
                    start_time = time.time()

                    while time.time() < start_time + move_time and not self.is_shutting_down():
                        self.cmd_vel_pub.publish(twist_msg)
                        time.sleep(0.1)

                    # Stop
                    stop_msg = Twist()
                    self.cmd_vel_pub.publish(stop_msg)

            return True

        except Exception as e:
            self.get_logger().error(f'Error executing navigate action: {e}')
            return False

    def execute_grip_action(self, parameters: dict) -> bool:
        """Execute a gripper grip action"""
        try:
            self.get_logger().info('Gripper closed (simulated)')
            return True
        except Exception as e:
            self.get_logger().error(f'Error executing grip action: {e}')
            return False

    def execute_release_action(self, parameters: dict) -> bool:
        """Execute a gripper release action"""
        try:
            self.get_logger().info('Gripper opened (simulated)')
            return True
        except Exception as e:
            self.get_logger().error(f'Error executing release action: {e}')
            return False

    def execute_speak_action(self, parameters: dict) -> bool:
        """Execute a speak action"""
        try:
            message = parameters.get('message', 'Hello')
            self.get_logger().info(f'Speaking: {message}')

            speech_msg = String()
            speech_msg.data = message
            self.speech_pub.publish(speech_msg)

            return True
        except Exception as e:
            self.get_logger().error(f'Error executing speak action: {e}')
            return False

    def execute_wait_action(self, parameters: dict) -> bool:
        """Execute a wait action"""
        try:
            duration = parameters.get('duration', 1.0)
            self.get_logger().info(f'Waiting for {duration} seconds')

            start_time = time.time()
            while time.time() < start_time + duration and not self.is_shutting_down():
                time.sleep(0.1)

            return True
        except Exception as e:
            self.get_logger().error(f'Error executing wait action: {e}')
            return False

    def execute_find_object_action(self, parameters: dict) -> bool:
        """Execute a find object action"""
        try:
            object_name = parameters.get('object', 'unknown')
            self.get_logger().info(f'Looking for object: {object_name}')

            # Use camera to look for object
            if self.latest_image is not None:
                # Simple color-based detection for demonstration
                hsv = cv2.cvtColor(self.latest_image, cv2.COLOR_BGR2HSV)

                # Detect red objects (for red cube example)
                lower_red1 = np.array([0, 50, 50])
                upper_red1 = np.array([10, 255, 255])
                lower_red2 = np.array([170, 50, 50])
                upper_red2 = np.array([180, 255, 255])

                mask1 = cv2.inRange(hsv, lower_red1, upper_red1)
                mask2 = cv2.inRange(hsv, lower_red2, upper_red2)
                mask = mask1 + mask2

                # Find contours
                contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

                if contours:
                    largest_contour = max(contours, key=cv2.contourArea)
                    if cv2.contourArea(largest_contour) > 500:  # Minimum area
                        self.get_logger().info(f'Found {object_name} in camera view')
                        return True

            self.get_logger().info(f'Could not find {object_name}')
            return False

        except Exception as e:
            self.get_logger().error(f'Error executing find object action: {e}')
            return False

    def speak_response(self, message: str):
        """Speak a response to the user"""
        try:
            speech_msg = String()
            speech_msg.data = message
            self.speech_pub.publish(speech_msg)
            self.get_logger().info(f'Spoken response: {message}')
        except Exception as e:
            self.get_logger().error(f'Error speaking response: {e}')

    def is_shutting_down(self) -> bool:
        """Check if the node is shutting down"""
        return not rclpy.ok()

def main(args=None):
    rclpy.init(args=args)
    node = FullVLAIntegration()

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        node.get_logger().info('Shutting down Full VLA Integration System')
    finally:
        # Stop robot and cleanup
        stop_msg = Twist()
        node.cmd_vel_pub.publish(stop_msg)

        # Stop audio
        node.audio.terminate()

        node.destroy_node()
        rclpy.shutdown()

# Include the Whisper and LLM components from previous modules
class WhisperTranscriber:
    def __init__(self):
        # Audio parameters
        self.chunk = 1024
        self.format = pyaudio.paInt16
        self.channels = 1
        self.rate = 16000
        self.record_seconds = 3

    def record_audio(self, filename="temp_audio.wav", record_duration=3):
        """Record audio from microphone"""
        print(f"Recording for {record_duration} seconds...")

        stream = self.audio.open(
            format=self.format,
            channels=self.channels,
            rate=self.rate,
            input=True,
            frames_per_buffer=self.chunk
        )

        frames = []
        for i in range(0, int(self.rate / self.chunk * record_duration)):
            data = stream.read(self.chunk)
            frames.append(data)

        print("Finished recording")

        stream.stop_stream()
        stream.close()

        wf = wave.open(filename, 'wb')
        wf.setnchannels(self.channels)
        wf.setsampwidth(self.audio.get_sample_size(self.format))
        wf.setframerate(self.rate)
        wf.writeframes(b''.join(frames))
        wf.close()

        return filename

    def transcribe_from_microphone(self):
        """Record and transcribe from microphone"""
        # Record audio
        audio_file = "temp_recording.wav"
        self.record_audio(audio_file, record_duration=5)

        # Simulate transcription (in real implementation, use Whisper API)
        simulated_commands = [
            "move forward one meter",
            "turn left and find the red cube",
            "stop the robot",
            "navigate to the kitchen",
            "pick up the object"
        ]
        import random
        text = random.choice(simulated_commands)

        # Clean up
        import os
        if os.path.exists(audio_file):
            os.remove(audio_file)

        return text

class LLMActionPlanner:
    def plan_actions(self, command: str, robot_state: dict):
        """Plan actions based on command and robot state"""
        # Simulate LLM response for demonstration
        # In real implementation, this would call an LLM API

        command_lower = command.lower()

        if "move forward" in command_lower:
            return {
                "actions": [
                    {
                        "action_type": "move",
                        "parameters": {"linear_x": 1.0, "duration": 2.0},
                        "description": "Move forward one meter"
                    }
                ]
            }
        elif "turn left" in command_lower:
            return {
                "actions": [
                    {
                        "action_type": "move",
                        "parameters": {"angular_z": 0.5, "duration": 2.0},
                        "description": "Turn left"
                    },
                    {
                        "action_type": "find_object",
                        "parameters": {"object": "red cube"},
                        "description": "Look for red cube"
                    }
                ]
            }
        elif "navigate" in command_lower or "go to" in command_lower:
            return {
                "actions": [
                    {
                        "action_type": "navigate",
                        "parameters": {"x": 2.0, "y": 1.0},
                        "description": "Navigate to specified location"
                    }
                ]
            }
        elif "stop" in command_lower:
            return {
                "actions": [
                    {
                        "action_type": "move",
                        "parameters": {"linear_x": 0.0, "angular_z": 0.0},
                        "description": "Stop the robot"
                    }
                ]
            }
        else:
            # Default response
            return {
                "actions": [
                    {
                        "action_type": "speak",
                        "parameters": {"message": f"I understand the command: {command}"},
                        "description": "Acknowledge command"
                    }
                ]
            }

if __name__ == '__main__':
    main()